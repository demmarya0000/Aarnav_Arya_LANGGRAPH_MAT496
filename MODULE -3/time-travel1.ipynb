{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Time Travel with Groq\n",
        "\n",
        "\n",
        "\n",
        "## Features Demonstrated:\n",
        "- Browsing state history\n",
        "- Replaying from checkpoints\n",
        "- Forking execution paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -q langgraph langchain_groq langgraph_sdk langgraph-prebuilt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"GROQ_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tools defined successfully!\n"
          ]
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "    \n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\n",
        "    \n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide a by b.\n",
        "    \n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a / b\n",
        "\n",
        "tools = [add, multiply, divide]\n",
        "# Using llama-3.3-70b-versatile which supports tool calling\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "print(\"Tools defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph created successfully!\n"
          ]
        }
      ],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import MessagesState, StateGraph, START\n",
        "from langgraph.prebuilt import tools_condition, ToolNode\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "# System message\n",
        "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
        "\n",
        "# Node\n",
        "def assistant(state: MessagesState):\n",
        "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
        "\n",
        "# Build graph\n",
        "builder = StateGraph(MessagesState)\n",
        "\n",
        "# Define nodes\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Define edges\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\"assistant\", tools_condition)\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "\n",
        "# Compile with memory\n",
        "graph = builder.compile(checkpointer=MemorySaver())\n",
        "\n",
        "print(\"Graph created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running agent...\n",
            "\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 2 and 3\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (p7cwh5gfr)\n",
            " Call ID: p7cwh5gfr\n",
            "  Args:\n",
            "    a: 2\n",
            "    b: 3\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: multiply\n",
            "\n",
            "6\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The result of multiplying 2 and 3 is 6.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Input\n",
        "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n",
        "\n",
        "# Thread\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Run the graph\n",
        "print(\"Running agent...\\n\")\n",
        "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
        "    event['messages'][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Browse History\n",
        "\n",
        "We can use `get_state` to look at the current state of our graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current State:\n",
            "Number of messages: 4\n",
            "Next steps: ()\n"
          ]
        }
      ],
      "source": [
        "current_state = graph.get_state({'configurable': {'thread_id': '1'}})\n",
        "print(\"Current State:\")\n",
        "print(f\"Number of messages: {len(current_state.values['messages'])}\")\n",
        "print(f\"Next steps: {current_state.next}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get all historical states:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total states in history: 5\n"
          ]
        }
      ],
      "source": [
        "all_states = [s for s in graph.get_state_history(thread)]\n",
        "print(f\"Total states in history: {len(all_states)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Look at a specific state (the one with human input):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "State at step -2:\n",
            "Messages: [HumanMessage(content='Multiply 2 and 3', additional_kwargs={}, response_metadata={}, id='0ead9494-349b-4089-b09c-7d8b04419f7b')]\n",
            "Next: ('assistant',)\n"
          ]
        }
      ],
      "source": [
        "print(\"State at step -2:\")\n",
        "print(f\"Messages: {all_states[-2].values['messages']}\")\n",
        "print(f\"Next: {all_states[-2].next}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replaying\n",
        "\n",
        "We can re-run our agent from any prior checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "State to replay:\n",
            "Messages: [HumanMessage(content='Multiply 2 and 3', additional_kwargs={}, response_metadata={}, id='0ead9494-349b-4089-b09c-7d8b04419f7b')]\n",
            "Next node: ('assistant',)\n",
            "Checkpoint ID: 1f0b19aa-b571-663a-8000-594d9e7baddc\n"
          ]
        }
      ],
      "source": [
        "to_replay = all_states[-2]\n",
        "print(\"State to replay:\")\n",
        "print(f\"Messages: {to_replay.values['messages']}\")\n",
        "print(f\"Next node: {to_replay.next}\")\n",
        "print(f\"Checkpoint ID: {to_replay.config['configurable']['checkpoint_id']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Replay from the checkpoint:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Replaying from checkpoint...\n",
            "\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 2 and 3\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (a1m1zwzff)\n",
            " Call ID: a1m1zwzff\n",
            "  Args:\n",
            "    a: 2\n",
            "    b: 3\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: multiply\n",
            "\n",
            "6\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The result of multiplying 2 and 3 is 6.\n"
          ]
        }
      ],
      "source": [
        "print(\"Replaying from checkpoint...\\n\")\n",
        "for event in graph.stream(None, to_replay.config, stream_mode=\"values\"):\n",
        "    event['messages'][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forking\n",
        "\n",
        "We can fork from a checkpoint by modifying the state and creating a new execution path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original message:\n",
            "[HumanMessage(content='Multiply 2 and 3', additional_kwargs={}, response_metadata={}, id='0ead9494-349b-4089-b09c-7d8b04419f7b')]\n"
          ]
        }
      ],
      "source": [
        "to_fork = all_states[-2]\n",
        "print(\"Original message:\")\n",
        "print(to_fork.values[\"messages\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Update the state with a new message (keeping the same ID to overwrite):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Forked config created:\n",
            "New checkpoint ID: 1f0b19aa-be49-6334-8001-c624f1486499\n"
          ]
        }
      ],
      "source": [
        "fork_config = graph.update_state(\n",
        "    to_fork.config,\n",
        "    {\"messages\": [HumanMessage(\n",
        "        content='Multiply 5 and 3',\n",
        "        id=to_fork.values[\"messages\"][0].id\n",
        "    )]}\n",
        ")\n",
        "print(\"Forked config created:\")\n",
        "print(f\"New checkpoint ID: {fork_config['configurable']['checkpoint_id']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify the state was updated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated first message:\n",
            "[HumanMessage(content='Multiply 5 and 3', additional_kwargs={}, response_metadata={}, id='0ead9494-349b-4089-b09c-7d8b04419f7b'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'p7cwh5gfr', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 373, 'total_tokens': 392, 'completion_time': 0.044432295, 'prompt_time': 0.035572388, 'queue_time': 0.049883892, 'total_time': 0.080004683}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--83141de2-4b4e-413e-9ef2-f400bae5ef1a-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'p7cwh5gfr', 'type': 'tool_call'}], usage_metadata={'input_tokens': 373, 'output_tokens': 19, 'total_tokens': 392})]\n"
          ]
        }
      ],
      "source": [
        "all_states = [state for state in graph.get_state_history(thread)]\n",
        "print(\"Updated first message:\")\n",
        "print(all_states[0].values[\"messages\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the forked version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running forked version...\n",
            "\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (p7cwh5gfr)\n",
            " Call ID: p7cwh5gfr\n",
            "  Args:\n",
            "    a: 2\n",
            "    b: 3\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: multiply\n",
            "\n",
            "6\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (3jpbd9g5s)\n",
            " Call ID: 3jpbd9g5s\n",
            "  Args:\n",
            "    a: 5\n",
            "    b: 3\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: multiply\n",
            "\n",
            "15\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The result of multiplying 5 and 3 is 15.\n"
          ]
        }
      ],
      "source": [
        "print(\"Running forked version...\\n\")\n",
        "for event in graph.stream(None, fork_config, stream_mode=\"values\"):\n",
        "    event['messages'][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the final state:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final state:\n",
            "Number of messages: 6\n",
            "Last message: The result of multiplying 5 and 3 is 15.\n",
            "Next steps: ()\n"
          ]
        }
      ],
      "source": [
        "final_state = graph.get_state({'configurable': {'thread_id': '1'}})\n",
        "print(\"Final state:\")\n",
        "print(f\"Number of messages: {len(final_state.values['messages'])}\")\n",
        "print(f\"Last message: {final_state.values['messages'][-1].content}\")\n",
        "print(f\"Next steps: {final_state.next}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
